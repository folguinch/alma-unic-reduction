# ALMA-UNIC pipeline

## Installation

Before start, the installation will also install/update the dependencies. These
include modular CASA. If you have a system wide modular CASA installation that
you want to keep, then it is recommended to install the pipeline within a 
virtual environment.

### General use

To install the ALMA-UNIC pipeline:

```bash
pip install git+git@github.com:alma-unic/alma-unic-reduction.git
```

For tasks requiring `tclean`, a monolithic CASA version is needed. If you prefer a
specific version of casa/mpicasa, then you can define the following environmental
variable:

```bash
export MPICASA="/path/to/mpicasa -n {0} /path/to/casa"
```

Note that only the paths to `casa` and `mpicasa` need to be modified, the number
of processes (after the `-n` flag) is evaluated internally from the command line
options.

### For developing

In general, to install in editable mode run:

```bash
# Clone to your desired directory
mkdir -p /my/preferred/directory
cd /my/preferred/directory
git clone git@github.com:alma-unic/alma-unic-reduction.git

# Install with pip in editable mode
cd unic-pipeline
pip install -e .
```

If you use `poetry` to manage dependencies, then follow their instructions for
installation with the lock file.

All dependencies are managed in the `pyproject.toml` file within the base
directory.

## Usage

Starting from the calibrated MS generated after the script for PI, run:

```bash
python -m unic_pipeline.unic --uvdata \path\to\calibrated\data1.ms \path\to\calibrated\data2.ms ... --cont \path\to\calibration\cont1.dat \path\to\calibration\cont2.dat ... --nproc N ...
```

The `--cont` flag indicates where the files with the continuum frequency ranges
are located. Note that the SPW numbering of 7m and 12m data is different, hence
the `cont.dat` files must be specified for each array. In the example above
`cont1.dat` is the contains the continuum frequencies for `data.ms`.

A single MS may contain more than 1 source. The pipeline extracts the data for
all the science sources and prepares the data for each one and performs the
imaging. There is no limit in the amount of data, but bear in mind that they are
processed in sequence.

The amount of processors for parallel imaging can be specified with the
`--nproc` flag. The verbosity level can be controlled with the `-v`, `--vv`,
`--vvv` flags (see help).

For help:

```bash
python -m unic_pipeline.unic -h
```

### Resuming and skipping steps

The pipeline is divided in several steps. In general, the program will check
whether the files generated by a step are already in the system and delete them
to perform the step again. However, these steps can be skipped if the files are
present with the `--resume` flag.

It is also possible to skip specific steps (see step list below), except for
those needed to collect needed information. To skip a step use the
`--skip step1 step2 ...` flag.

*WARNING: the following has not been tested thoroughly.*
An initial run will generate a configuration file with parameters used for each
step. It is possible to resume the processing or start from zero with
configuration files with:

```bash
python -m unic_pipeline.unic --configfiles \path\to\config1.ms \path\to\config2.ms ... --nproc N ...
```

## Pipeline steps implemented

The following steps have been implemented (for more details see `unic.py` documentation):

- `split`: It collects the data and split the science data.
- `dirty_cubes`: Computes dirty cubes of the split data.
- `continuum`: Generates the continuum MS files.
- `contsub`: Produces continuum subtracted visibilities.
- `combine_arrays`: Combine 7m and 12m data if possible.
- `clean_cont`: Produce CLEAN continuum images.
- `clean_cubes`: produce CLEAN continuum subtracted cubes.

Continuum and cube cleaning are being tested.
